%% LaTeX2e class for student theses
%% sections/evaluation.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.3, 2018-04-17

\chapter{Evaluation}
\label{ch:Evaluation}

The evaluation of this work
mainly focuses on the quality and usefulness of the resulting schemas.
Unfortunately, without being able to reliably validate datasets against schemas –
for example, to verify whether an item for an author
matches a schema inferred from fifty other authors –
it is not possible to objectively assess the quality of the inferred schemas.
However, inspecting the schemas manually
can still give insights on their quality (\cref{sec:Evaluation:quality}),
as well as the quality of the underlying data,
and it is also possible to extract smaller subsets from the full schemas,
which can then be used for validation (\cref{sec:Evaluation:extraction}).
In these ways, the schemas can still be useful even though they can’t be used for validation directly.
Additionally, the runtime of the inference process is examined in \cref{sec:Evaluation:duration}.

% TODO find a better title for this section
\section{Schema Quality}
\label{sec:Evaluation:quality}

While the schema quality cannot be assessed objectively without verifying them,
it is possible to simply look at the schemas and see if they “make sense” on their own.
Generally, the inferred schemas are much too large to read the entire schema,
but one can select the shapes for individual classes
(randomly or by searching for the item IDs of specific classes)
and check their type links.
For example, \cref{listing:13th-riigikogu-Q5} shows the shape for the class \QL{Q5}{human}
from the schema inferred from 50 members of the 13th Riigikogu (the Estonian parliament).
In textual form, it means that a human should have the following statements:
\begin{itemize}
\item Member of any number of political parties, where the values are political parties.
\item Any number of occupations, where the values are positions.
  This likely reflects the fact that the input data only contained politicians –
  other schemas often include other possible classes for this property,
  such as “occupation” or “profession”.
\item Any number of spoken, written or signed languages, where the values are languages.
\item Optionally, a name in the subject’s native language.
\item Any number of awards received, where the values are classes of awards.
  The target class, “class of award”, is an artifact of the not completely consistent modeling of awards in Wikidata:
  there is sometimes confusion about the relationship between different “levels” of awards,
  such as “award”, “peace prize”, “Nobel Prize”, “Nobel Peace Prize”, “2018 Nobel Peace Prize”,
  and whether they should be instances or subclasses of each other.
\item Optionally, a place of birth, where the value is a human settlement.
\item Optionally, a sex or gender, where the value is a sex of humans or a gender.
  The two target classes are the result of the two most common gender items in Wikidata,
  \QL{Q6581097}{male} and \QL{Q6581072}{female},
  each having several \PL{P31}{instance of} statements.
\item Optionally, a country of citizenship, where the value is a political territorial entity.
\item Optionally, a Wikimedia Commons category.
\item Any number of positions held by the subject, where the values are positions.
\item Optionally, a date of birth.
\item Any number of places where the subject was educated, where the values are educational institutions.
\item Optionally, a family name, where the value is a family name.
\item Optionally, a given name, where the value is a given name.
\item Any number of work locations, where the values are human settlements.
\end{itemize}

\begin{lstfloat}[t]
\begin{lstlisting}[language=sparql]
wd:Q5 { # & wd:Q215627
  wdt:P102 @wd:Q7278*;
  wdt:P106 @wd:Q4164871*;
  wdt:P1412 @wd:Q34770*;
  wdt:P1559 rdf:langString?;
  wdt:P166 @wd:Q38033430*;
  wdt:P19 @wd:Q486972?;
  (
    wdt:P21 @wd:Q4369513? |
    wdt:P21 @wd:Q48277?
  );
  wdt:P27 @wd:Q1048835?;
  wdt:P373 xsd:string?;
  wdt:P39 @wd:Q4164871*;
  wdt:P569 xsd:dateTime?;
  wdt:P69 @wd:Q2385804*;
  wdt:P734 @wd:Q101352?;
  wdt:P735 @wd:Q202444?;
  wdt:P937 @wd:Q486972*
}
\end{lstlisting}
\caption{Excerpt of a schema inferred from 50 members of the 13th Riigikogu.}
\label{listing:13th-riigikogu-Q5}
\end{lstfloat}

Aside from some quirks of the input data, all of these seem reasonable to me.
The most obvious missing predicates are date and place of death,
which is again due to the input data set:
all the members of the current Estonian parliament are alive,
and while one might expect to see dates of death on some items they link to,
the fact that currently only 33 parents of members of the 13th Riigikogu are known to Wikidata
makes it seem plausible enough that there simply happened to be no dead humans in the full input data.

As the input data sets get larger,
the schemas also tend to grow:
in the number of shapes (classes),
the number of predicates in each shape,
and also in the number of possible classes for a predicate.
For example, before simplification,
the \QL{Q5}{human} shape from the schema inferred from the set of US presidents
lists \emph{nine} possible classes for someone’s \PL{P735}{given name}:
\begin{itemize}
\item A generic \QL{Q202444}{given name},
  such as \QL{Q18396847}{Boylston} in Thomas Boylston Adams, the third son of President John Adams.
  (The more common given names generally have a more specific class,
  usually one of the next three listed here.)
\item A \QL{Q12308941}{male given name},
  such as \QL{Q677191}{James} in President James A.~ Garfield.
\item A \QL{Q11879590}{female given name},
  such as \QL{Q644599}{Ida} in Ida Stover Eisenhower, mother of President Eisenhower.
\item A \QL{Q3409032}{unisex given name},
  such as \QL{Q564684}{Anne} in Nancy Reagan (born Anne Frances Robbins), wife of President Reagan.
  (“Anne” is a female given name in English,
  but sometimes used as a male given name in the Netherlands or France,
  and therefore classified as both female and unisex in Wikidata, depending on language.)
\item A \QL{Q1243157}{compound given name},
  such as \QL{Q16275947}{George Washington} in George Washington Adams, the first son of President John Quincy Adams.
\item A \QL{Q1130279}{hypocorism} (a diminutive form of a name),
  such as \QL{Q2165388}{Ron} in Ron Reagan, son of President Reagan.
\item A \QL{Q108709}{diminutive},
  such as \QL{Q4166211}{Jimmy} in President Jimmy Carter.
  (Arguably, that item should be an instance of the aforementioned hypocorism class
  instead of the more generic diminutive class, which also includes non-names like “auntie”.)
\item \QL{Q19803443}{Initials instead of given names},
  such as \QL{Q19803518}{S.} in President Harry S.~Truman (not an abbreviation).
\item A \QL{Q101352}{family name},
  such as \QL{Q2800825}{Simpson} in President Ulysses S.~Grant
  (the “S.” is sometimes believed to be short for “Simpson”, his mother’s maiden name).
  This is clearly an error: even if the “S.” does stand for “Simpson”, which Grant denied,
  the correct item to use would be the given name \Q{Q50876620}, not the family name \Q{Q2800825}.
\end{itemize}

If the problems pointed out above were to be fixed,
all the remaining classes could be merged into the generic \QL{Q202444}{given name} class,
as in the earlier schema.
However, due to the presence of \QL{Q101352}{family name},
the best common superclass is instead \QL{Q10856962}{anthroponym},
the common superclass of given and last names,
and the \QL{Q108709}{diminutive} class remains unmerged,
as it is not a subclass of any kind of name.

One can continue to pick apart the generated schemas in this manner for hours,
and I have done so while working on this thesis
in order to find problems and possible improvements in the inference process.
Three insights emerge:

\begin{enumerate}
\item The simplification step is vital for readable schemas.
  The schemas are no less correct without simplification as far as the input data set is concerned,
  but if the numerous subclasses in Wikidata’s hierarchy are never merged,
  the schemas become exceedingly tedious to read for humans,
  and they will also often match other data sets less well
  if the other data sets involve subclasses that were not encountered in the original input data.

  This can be seen in some of the older jobs of the Wikidata Shape Expressions Tool:
  if there is a \lstinline[language=java]{StackOverflowError} in the standard error of the inference process,
  it means that RDF2Graph crashed during the simplification step
  and the \gls{shex} export ran on the original, unsimplified graph.
  (Later, the inference step was made more robust,
  which is why newer jobs do not have this problem.)

\item Sometimes, biases in the schema are visible, due to biases in the input data set.
  (In one particularly egregious case,
  a schema proclaimed that a given name must always be a \emph{male} given name,
  since there had been no women with given names in the input data set.)
  One can presume that at other times biases in the schema are not visible,
  which does not mean that they are not present. % TODO abbreviate to “not visible, but still present”?
  To arrive at useful schemas,
  care must be taken when selecting the input data set,
  and the results must be viewed critically. % TODO is “viewed” the best verb here?

\item Sometimes, the schema reflects errors in the input data,
  as in the case above where a family name item was used for a given name.
  This can often be traced back to confusion between several items with similar labels.
  Such errors are usually visible in the resulting schema if one takes the time to read it,
  though they are not always obvious due to the simplification
  (as when the classes for given and family names were merged into anthroponyms above).
\end{enumerate}

% TODO “testimonials” from others?

% TODO find a better title for this section
\section{Manual schema extraction}
\label{sec:Evaluation:extraction}

One way to make use of the inferred schemas
even though they cannot be used for validation directly
is to extract a smaller subset of the schema manually,
then validating items against that.
If desired, that schema can then be further altered and augmented as deemed necessary,
making the original schema the basis of a manually curated one.

Generally, to extract parts of the schema,
one starts with a basic shape for the input entities
(e.~g. \QL{Q5}{human} if the schema was inferred from a set of humans),
copies it into the reduced schema,
and repeats this procedure for all shapes (classes) mentioned by that shape which had not been copied yet.
Predicates which link to shapes that should not be included can be dropped when copying a shape:
for example, the \PL{P27}{country of citizenship} shape should be dropped
if one is not interested in including shapes for countries, states etc. in the reduced schema.
Some predicates may also need to be moved between shapes,
or the target classes may need to be adjusted,
if the results of the automatic simplification are not satisfactory,
e.~g. if unrelated classes were merged into a very fundamental base class like \QL{Q937228}{property}
despite the changes in \cref{subsec:RDF2Graph+Wikidata:Wikidata:simplification}.

\begin{lstfloat}[ht]
\begin{sublstfloat}[t]{0.45\textwidth}
\begin{lstlisting}[language=sparql,showlines=true]
wd:Q12136 {
  wdt:P1478 @wd:Q16521?;
  wdt:P1748 xsd:string?;
  wdt:P2572 xsd:string?;
  wdt:P373 xsd:string?;
  wdt:P667 xsd:string?;
  wdt:P828 @wd:Q16521?
}

wd:Q16521 {
  wdt:P1542 @wd:Q12136?;
  wdt:P171 @wd:Q16521*;
  wdt:P225 xsd:string?;
  wdt:P373 xsd:string?;
  wdt:P935 xsd:string?
}
















\end{lstlisting}
\caption{Schema for diseases, based on \wdsiJob{37}.}
\label{listing:zika}
\end{sublstfloat}
\begin{sublstfloat}[t]{0.45\textwidth}
\begin{lstlisting}[language=sparql]
wd:Q11424 {
  wdt:P1040 @wd:Q5*;
  wdt:P1431 @wd:Q5*;
  wdt:P154 .?;
  wdt:P161 @wd:Q5*;
  wdt:P162 @wd:Q5*;
  wdt:P175 @wd:Q5?;
  wdt:P1809 @wd:Q5*;
  wdt:P2130 xsd:decimal?;
  wdt:P2142 xsd:decimal?;
  wdt:P2515 @wd:Q5*;
  wdt:P2554 @wd:Q5*;
  wdt:P2769 xsd:decimal?;
  wdt:P3092 @wd:Q5*;
  wdt:P3174 @wd:Q5*;
  wdt:P3300 @wd:Q5?;
  wdt:P344 @wd:Q5*;
  wdt:P373 xsd:string?;
  wdt:P57 @wd:Q5*;
  wdt:P58 @wd:Q5*;
  wdt:P725 @wd:Q5*;
  wdt:P86 @wd:Q5*
}

wd:Q5 {
  wdt:P22 @wd:Q5?;
  wdt:P25 @wd:Q5?;
  wdt:P26 @wd:Q5*;
  wdt:P40 @wd:Q5*;
  wdt:P569 xsd:dateTime?;
  wdt:P570 xsd:dateTime?
}
\end{lstlisting}
\caption{Schema for films, based on \wdsiJob{36}.}
\label{listing:films}
\end{sublstfloat}
\caption{Two schemas manually extracted from automatically inferred ones.}
\label{listing:extracted}
\end{lstfloat}

\Cref{listing:extracted} shows two schemas that were manually extracted in this fashion
from schemas inferred by the Wikidata Shape Expressions Inference Tool.
\Cref{listing:zika} was extracted from a schema
inferred from 100 scientific articles about the Zika virus,
and contains shapes for the classes \QL{Q12136}{disease} and \QL{Q16521}{taxon}:
diseases may be caused (indirectly or immediately) by taxa,
and taxa may effect diseases and have any number of parent taxa.
\Cref{listing:films} was extracted from a schema
inferred from the set of films which won three or more Academy Awards (“Oscars”) % TODO glossary? ™?
and contains shapes for the classes \QL{Q11424}{film} and \QL{Q5}{human}:
films have human directors, editor, cast members, screenwriters, etc.,
and humans may have parents, any number of spouses and children, and dates of birth and death.

These schemas are simple enough that they can be validated using the “Simple Online Validator”
at \url{https://rawgit.com/shexSpec/shex.js/wikidata/doc/shex-simple.html},
which automatically downloads all the required data from Wikidata.
Validating 100 arbitrary diseases
against the shape for \QL{Q12136}{disease} from \cref{listing:zika} mostly yields positive results,
with only four violations:
the items \QL{Q366964}{X-linked adrenoleukodystrophy},
\QL{Q580285}{Morquio Syndrome} and \QL{Q2200359}{Sanfilippo syndrome}
have more than one \PL{P1748}{NCI Thesaurus ID} statement, while the schema says a disease may only have zero or one such IDs,
and the item \QL{Q895930}{nodding disease} fails validation because it has two \PL{P828}{has cause} statements,
\QL{Q8084905}{autoimmune disease} and \QL{Q1601794}{parasitic infectious diseases},
whereas the schema says a disease may only have zero or one causes.
(The schema also says that those causes should be taxa, not other diseases,
but the shape for taxa is sufficiently lax that these diseases match it.)
Validating 100 arbitrary films
against the shape for \QL{Q11424}{film} from \cref{listing:films} also yields positive results with only a single violation –
\QL{Q185143}{Detective Conan} has two \PL{P154}{logo image} statements (French and Japanese)
whereas the schema says it should only have one.

A second look at the schemas makes it clear why there are so few violations:
the cardinalities for all predicates, without exception,
are either \lstinline{?} (“zero or one”) or \lstinline{*} (“any number”).
A completely empty item with no statements at all
will match all four shapes in \cref{listing:extracted}.
This is likely an artifact of the schema extraction procedure,
which in this case was extremely selective of shapes
to ensure that the schema would fit on one page of this document,
and therefore only included very few shapes,
each of which had had a high number of examples in the input data set:
enough that, for any property,
there was an example item missing statements for that property,
forcing the lower boundary of the cardinality to be zero.
Overall, the sample schemas investigated in \cref{sec:jobs-over-various}
have \SIrange{17}{34}{\percent} of non-optional predicates,
so if one includes some more shapes during the extraction process,
they are bound to include some required predicates soon.
(Alternatively, the cardinality of some predicates could be manually raised during the extraction.)

Generally, these extracted schemas appear to be useful to find some mistakes,
though their usefulness % TODO duplicate word, we already used “useful” just above
can be increased by extracting larger parts of the schemas
and by further refining them according to one’s personal experience with the data model.
A useful side effect is that the kind of careful inspection of the schema
which is necessary to extract useful parts of it
will also likely find some problems in the schema where they exist,
pointing at problems % TODO another duplicate word
in the original input data set,
as demonstrated in \cref{sec:Evaluation:quality}.

% TODO find a better title for this section
\section{Duration of the inference process}
\label{sec:Evaluation:duration}

While attempts to validate data against the inferred schema
suffer due to the size of the input data and the schemas,
no such problems appear to plague the inference process,
which, while slow, has a more reliable runtime.
To some degree, this was already apparent in the execution times
of the various jobs that were run on the Wikidata Shape Expressions Inference Tool:
ranging from five or ten minutes to several hours,
they roughly follow the size of the input data linearly.

However, the data from the Wikidata Shape Expressions Inference tool
is not without its problems:
as the tool was tested with different jobs,
various problems were discovered and subsequently fixed
(some of them described in more detail earlier, others too minor to be worth mentioning),
so the runtimes available from the tool apply to a range of software versions,
with several instances of the same job being repeated (to verify a software fix)
with highly different runtimes.
Therefore, a subset of the tool’s jobs was selected
and repeated locally, with a single software version,
to get more reliable execution times.
\Cref{sec:jobs-over-various} contains graphs of runtime over various factors,
which are explained in more detail in the following paragraphs,
each with three different functions fitted to the data:
a simple linear function $a+bx$,
a quadratic function $a+bx+cx^2$,
and a power function $a+bx^c$.
Each figure contains two subfigures,
one for the full data and one with two outlier records removed –
see the text in the appendix for details.

The most obvious possible relation is
to directly compare the runtime to the number of entities selected by the user’s query,
as shown in \cref{fig:jobs-over-entities}.
However, it is clear from the graphs that there is no direct relation
between the number of entities and the runtime,
which is not surprising because the amount of work that RDF2Graph has to do
highly depends on the size of the entities,
as well as the number of entities indirectly selected as the values of statements on the original entities (and their size).

Instead, a much more sensible relation is the total amount of input data:
the number of triples which Fuseki serves to RDF2Graph
(that is, the number of lines in the N-Triples file). % TODO \cref background?
As can be seen in \cref{fig:jobs-over-triples},
this results in a fairly linear relation,
especially if two outliers are removed,
in which case the functions fit the data very well.
However, with the outliers included the fit is much less satisfactory.

\begin{lstfloat}[b]
\begin{lstlisting}[language=awk]
BEGINFILE {
  count = 0;
}

$2 == "<http://www.wikidata.org/prop/direct/P31>" {
  count++;
}

ENDFILE {
  print FILENAME, count
}
\end{lstlisting}
\caption{GNU AWK script to count the number of \PName{wdt:P31} triples in the input.}
\label{listing:P31}
\end{lstfloat}

\begin{lstfloat}[b]
\begin{lstlisting}[language=awk]
BEGINFILE {
  count = 0;
  delete classes;
}

$2 == "<http://www.wikidata.org/prop/direct/P31>" {
  classes[$3]++
}

ENDFILE {
  for (class in classes)
    count++;
  print FILENAME, count;
}
\end{lstlisting}
\caption{GNU AWK script to count distinct classes in the input.}
\label{listing:classes}
\end{lstfloat}

Since RDF2Graph heavily relies on type information,
another possible factor for execution time
is just the number of \PName{wdt:P31} triples in the input,
rather than the overall number of triples.
(Recall that \PL{P31}{instance of} is the Wikidata property linking an item to its class.)
% TODO a) is that “recall” appropriate? b) was that even clearly explained yet?
The number of \PName{wdt:P31} triples was counted
using the GNU AWK snippet found in \cref{listing:P31},
and the result is shown in \cref{fig:jobs-over-P31s}:
the functions are more linear and fit the data better, both with and without outliers.
However, with outliers included the fit is still not completely satisfactory.

Alternatively, instead of counting \PName{wdt:P31} triples
it is also possible to count the distinct number of classes in the input data.
Classes were counted using the GNU AWK snippet found in \cref{listing:classes},
and the result is shown in \cref{fig:jobs-over-classes}:
the functions are significantly less linear now
(though this is not visible in the function equations shown in the graphs, due to rounding),
but they finally fit the data well without excluding the outliers:
in fact, the graph with outliers has slightly better fits than the one without outliers.

Two conclusions are possible from this:
the execution time could derive linearly from the number of input triples or \PName{wdt:P31} triples,
or it could derive nonlinearly from the number of input classes.
The first conclusion has no satisfactory explanation for the outliers
which need to be excluded to get good function fits,
whereas the second conclusion requires no cherry-picking in the data
but lacks an explanation for the nonlinear runtime.
% TODO is this undecidedness between the conclusions acceptable?

One might think that these relations are not very useful
because they only predict the duration of the whole process partway through the process.
However, all of the possible predictor variables – % TODO “predictor variables” feels weird – regressors? independent variables?
number of entities, number of triples, number of \PName{wdt:P31} triples, number of distinct classes –
can be determined after the data download step,
which is both the very first step in the whole process
and also does not take a long time:
in the jobs listed in \cref{sec:jobs-over-various},
the download never takes more than twenty seconds,
and for most jobs it finishes within about five seconds.
This means that it takes less than half a minute
to be able to mostly predict the duration of the full job, which can be several hours.
This suggests two possible future improvements
for the Wikidata Shape Expressions Inference Tool:
to report to the user how long a job is expected to take,
as soon as the download step has finished;
and to reject jobs which resulted in too much data,
and are expected to take far too long to be tolerable.
Currently, the tool merely suggests to its users
that their queries should not select more than about fifty entities,
but does not implement any kind of hard limit beyond that.
